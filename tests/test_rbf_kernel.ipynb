{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import grad\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from stein_mpc.kernels import GaussianKernel, ScaledGaussianKernel\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "# from D. Wang and Q. Liu SVGD paper (https://github.com/dilinwang820/Stein-Variational-Gradient-Descent/blob/master/python/svgd.py)\n",
    "def baseline(X, Y, h):\n",
    "    gamma = 0.5 / h.numpy() ** 2\n",
    "    Kxy = rbf_kernel(X, Y, gamma)\n",
    "\n",
    "    dx_Kxy = -np.matmul(Kxy, X)\n",
    "    sum_Kxy = np.sum(Kxy, axis=1)\n",
    "    for i in range(X.shape[1]):\n",
    "        dx_Kxy[:, i] = dx_Kxy[:, i] + np.multiply(X[:,i], sum_Kxy)\n",
    "    dx_Kxy = dx_Kxy / (h**2)\n",
    "    return Kxy, dx_Kxy\n",
    "\n",
    "# from D. Wang and Q. Liu Matrix SVGD paper (https://github.com/dilinwang820/matrix_svgd/blob/master/2d_toy/code/kernel.py)\n",
    "def scaled_baseline(X, Y, M, h):\n",
    "    n,d = X.shape\n",
    "    diff = X[:, None, :] - Y[None, :, :]\n",
    "    Mdiff = torch.matmul(diff, M)\n",
    "    K = torch.exp(-torch.sum(Mdiff * diff, dim=-1)/(2.0 * h ** 2))\n",
    "    gradK = -Mdiff * K[:,:,None] / h ** 2\n",
    "    return K, gradK.sum(0)\n",
    "\n",
    "def naive(X, Y, h):\n",
    "    batch, _ = X.shape\n",
    "    Kxy = torch.empty(batch, batch)\n",
    "    dx_Kxy = torch.zeros_like(X)\n",
    "    dy_Kxy = torch.zeros_like(X)\n",
    "    for i in range(batch):\n",
    "        for j in range(batch):\n",
    "            # using row vectors\n",
    "            diff = X[i] - Y[j]\n",
    "            norm = diff @ diff.T\n",
    "            Kxy[i, j] = (-0.5 / h ** 2 * norm).exp()\n",
    "            dx_Kxy[i] += -diff * Kxy[i, j] / h ** 2\n",
    "            dy_Kxy[j] += diff * Kxy[i, j] / h ** 2\n",
    "    return Kxy, dx_Kxy, dy_Kxy\n",
    "\n",
    "rbf = GaussianKernel()\n",
    "scaled_rbf = ScaledGaussianKernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH, DIM = [20, 5]\n",
    "torch.set_default_dtype(torch.double)\n",
    "X = torch.randn(BATCH, DIM)\n",
    "Y = torch.randn(BATCH, DIM)\n",
    "M = torch.eye(DIM)\n",
    "h = torch.tensor(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "508 µs ± 710 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "44.9 ms ± 102 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "149 µs ± 153 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "163 µs ± 181 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit baseline(X, Y, h)\n",
    "%timeit naive(X, Y, h)\n",
    "%timeit rbf(X, Y, h)\n",
    "%timeit scaled_rbf(X, Y, M, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All covariance matrices equal:  True\n",
      "All derivatives w.r.t. X equal to baseline:  False\n",
      "All derivatives w.r.t. X equal to scaled baseline:  False\n",
      "All derivatives w.r.t. X equal to naive:  True\n"
     ]
    }
   ],
   "source": [
    "Kb, dKb = baseline(X, Y, h)\n",
    "Ksb, dKsb = scaled_baseline(X, Y, M, h)\n",
    "Kn, dKxn, dKyn = naive(X, Y, h)\n",
    "Kr, dKr = rbf(X, Y, h)\n",
    "Ks, dKs = scaled_rbf(X, Y, M, h)\n",
    "\n",
    "Xg = X.clone().requires_grad_()\n",
    "Yg = Y.clone().requires_grad_()\n",
    "dKag = grad(scaled_baseline(Xg, Yg, M, h)[0], (Xg, Yg), torch.ones(20, 20))  # autograd\n",
    "\n",
    "print(\"All covariance matrices equal: \", np.allclose(Kb, Ksb) and np.allclose(Kb, Kn) and np.allclose(Kb, Kr) and np.allclose(Kb, Ks))\n",
    "print(\"All derivatives w.r.t. X equal to baseline: \", np.allclose(dKb, dKr) and np.allclose(dKb, dKs) and np.allclose(dKb, dKag[0]))\n",
    "print(\"All derivatives w.r.t. X equal to scaled baseline: \", np.allclose(dKsb, dKr) and np.allclose(dKsb, dKs) and np.allclose(dKsb, dKag[0]))\n",
    "print(\"All derivatives w.r.t. X equal to naive: \", np.allclose(dKxn, dKr) and np.allclose(dKxn, dKs) and np.allclose(dKxn, dKag[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The gradient of the Kernel on the reference code seems to have a mistake on the aggregating axis. When the inputs are the same (X, X'), this results in a opposite sign to that of the autograd and naive implementation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stein",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18311e1eab8eea7ccc6304a31eeccf4665b9bc2078a2fef2d13a4bf50e562f61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
